{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSST B10m 2024: GPT-2 transformer exercise\n",
    "Daniel Warren\n",
    "May 2024\n",
    "# About this notebook\n",
    "This notebook is for exploring the smallest version of GPT-2 (124M parameters), described in the 2019 report by OpenAI [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).\n",
    "\n",
    "This model is extremely small and limited in performance compared to state-of-the-art LLMs. However, it has several advantages for use here:\n",
    "* it has a permissive open source licence\n",
    "* it is relatively small (~500MB) and lightweight, enabling it to be run in a Binder environment\n",
    "* it is simple enough that the forward-pass (i.e. predicting a text completion) has been implemented in ~60 lines of numpy code as [picoGPT](https://github.com/jaymody/picoGPT)\n",
    "* the network architecture used by OpenAI is believed to remain very similar to this, at least as far as GPT-3.5, except scaled-up\n",
    "\n",
    "All of the 'clever' functionality of this notebook uses functions from picoGPT. See [this blog post](https://jaykmody.com/blog/gpt-from-scratch/) for a detailed explanation by that code's author.\n",
    "\n",
    "Daniel Warren\\\n",
    "May 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add picoGPT to path so functions can be imported like a module\n",
    "import os, sys\n",
    "pwd = os.getcwd()\n",
    "picogpt_path = os.path.join(pwd,'picoGPT')\n",
    "sys.path.append(picogpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Text completion\n",
    "Start by running the code below (making changes if you want) to complete a text sequence. You'll find that its 'abilities' are quite limited - it rarely provides correct factual information and is very prone to repetition after only a few tokens.\n",
    "\n",
    "Note especially that this model is only trained to complete arbitrary text, not 'instruction-tuned' to behave as a ChatGPT-style assistant. if you ask a question it might continue as though it were a rhetorical question in an essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from picoGPT\n",
    "from picoGPT.gpt2_pico import main as complete_text\n",
    "from ipywidgets import HTML\n",
    "\n",
    "prompts = [\"The director of the film Titanic was\",\n",
    "           \"Water boils at\",\n",
    "           \"How many items are there in a dozen?\",\n",
    "           \"The uses of artificial intelligence in medical physics include\"]\n",
    "\n",
    "for (i,prompt) in enumerate(prompts):\n",
    "    completion = complete_text(prompt,n_tokens_to_generate=20)\n",
    "    display(HTML(f'<pre><span style=\"color:red\">{prompt}</span><span style=\"color:blue\">{completion}</span>...</pre>'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Tokenizer\n",
    "The first part of the inference process is tokenization - converting the input text string into a sequence of numbers representing word parts.\n",
    "\n",
    "Use the file browser on the left-hand side to navigate into the folder 'models/124M'. The files in here contain all of the pre-trained parameters/weights necessary to predict text in this model.\n",
    "\n",
    "Two files are used by the tokenizer (here called 'encoder'), which converts a text string into a sequence of tokens:\n",
    "  * **encoder.json** contains the mappings of text parts (keys in the JSON file) to token IDs (values)\n",
    "    - You can open this in the Jupyter text editor. The file starts with tokens that represent numbers - you'll need to go to line ~1100 or so before you start seeing text, and even further down to see significant parts of words.\n",
    "  * **vocab.bpe** is used for byte-pair encoding: iteratively merging commonly-occuring pairs of tokens to a single value\n",
    "    - This can also be opened in the Jupyter text editor. In both files you will see many tokens starting with 'Ä ' - this indicates a space prior to that token.\n",
    "\n",
    "Run the code below (making changes if you want) to see how the tokenizer splits up various strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from picoGPT.encoder import get_encoder\n",
    "tokenizer = get_encoder(\"124M\", \"models\")\n",
    "\n",
    "print (\"TOKENIZATION EXAMPLES:\")\n",
    "# Tokenizer examples\n",
    "prompts = [\"three consecutive words\",\"three\",\"consecutive\",\"words\",\" consecutive\"]\n",
    "all_tokens = set()\n",
    "for prompt in prompts:\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    print(f\"'{prompt}' tokenizes to {tokens}\")\n",
    "    all_tokens = all_tokens.union(tokens)\n",
    "\n",
    "token_mapping = {t:tokenizer.decode([t]) for t in sorted(all_tokens)}\n",
    "\n",
    "print (\"\\nTOKEN MAPPINGS:\")\n",
    "for (k,v) in token_mapping.items():\n",
    "    print(f\"{k}: '{v}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hyper-parameters\n",
    "The remaining files in the 'models/124M' directory are not directly human-readable. They comprise a 'checkpoint' from the Tensorflow machine-learning framework, including all of the pre-trained weights/parameters necessary to run the GPT-2 model.\n",
    "\n",
    "The code below reads in the parameters and displays key network hyperparameters:\n",
    "* **n_vocab** is the size of the tokenizer's vocabulary (the number of word/text parts that have been assigned a unique token ID)\n",
    "* **n_ctx** is the size of the context (the maximum length of text that can be processed, in units of tokens)\n",
    "* **n_embd** is the size of the embedding (the number of directions in the arbitrary, learned, coordinate-system (or 'latent space') that the model uses to represent token meaning and position)\n",
    "* **n_head** is the number of attention heads per layer (the attention mechanism is applied in parallel N times on different linear transformations of the embeddings, with those transformations being learned parameters)\n",
    "* **n_layer** is the number of layers in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from picoGPT.utils import load_encoder_hparams_and_params\n",
    "_, hparams, params = load_encoder_hparams_and_params(\"124M\",\"models\")\n",
    "\n",
    "print (\"NETWORK HYPERPARAMETERS:\")\n",
    "for (k,v) in hparams.items():\n",
    "    print(f\"{k}: '{v}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Parameters/weights\n",
    "The learned parameters ('weights') of the model are the results of the training process for GPT-2.\n",
    "### 4.1 Token Embedding\n",
    "* **wte** is the token embedding matrix, mapping each token in the vocabulary to a vector of length n_embd - a multi-dimensional 'latent space'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "prompts = [\" dog\",\" cat\",\" horse\",\" car\",\" bus\",\" train\"]\n",
    "\n",
    "x_vals = []\n",
    "y_vals = []\n",
    "\n",
    "rows = {}\n",
    "\n",
    "for (i,prompt_a) in enumerate(prompts):\n",
    "    for (j,prompt_b) in enumerate(prompts):\n",
    "            \n",
    "            # Note that these will only take the first token if the input encodes to multiple token\n",
    "            embedding_a = tokenizer.encode(prompt_a)\n",
    "            embedding_b = tokenizer.encode(prompt_b)\n",
    "            if (len(embedding_a) > 1) or (len(embedding_b) > 1):\n",
    "                print(\"WARNING: One or more prompts contains >1 token. Only first token will be used.\")\n",
    "            token_a_embedding = params['wte'][embedding_a[0]]\n",
    "            token_b_embedding = params['wte'][embedding_b[0]]\n",
    "\n",
    "            similarity = np.dot(token_a_embedding/np.sqrt(np.sum(token_a_embedding**2)),\n",
    "                                token_b_embedding/np.sqrt(np.sum(token_b_embedding**2)))\n",
    "\n",
    "            if prompt_a not in rows.keys():\n",
    "                rows[prompt_a] = pd.Series(index=[prompt_a])\n",
    "            rows[prompt_a][prompt_b] = similarity\n",
    "\n",
    "df = pd.DataFrame([rows[prompt_a] for prompt_a in prompts],index=prompts)\n",
    "\n",
    "ax = plt.axes()\n",
    "sns.heatmap(df, annot=True, fmt=\".2f\", linewidths=.5,cmap=\"jet\",ax=ax)\n",
    "plt.title(\"Cosine similarity of token embeddings\")\n",
    "plt.show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Position Embedding\n",
    "Attention doesn't inherently consider spatial relationships (unlike e.g. convolution) so this must be explicitly added.\n",
    "* **wpe** is the position encoding matrix, mapping each token in the context to a vector of length n_embd in the same latent space as **wte**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_0_embedding = params['wpe'][0]\n",
    "\n",
    "for (p,i) in enumerate([0,256,512,768]):\n",
    "    pos_i_embedding = params['wpe'][i]\n",
    "    similarities = []\n",
    "    for j in range(1023):\n",
    "        pos_j_embedding = params['wpe'][j]\n",
    "        similarity = np.dot(pos_i_embedding/np.sqrt(np.sum(pos_i_embedding**2)),\n",
    "                            pos_j_embedding/np.sqrt(np.sum(pos_j_embedding**2)))\n",
    "        similarities.append(similarity)\n",
    "    plt.subplot(2,2,p+1)\n",
    "    plt.plot(similarities)\n",
    "    plt.xlabel('Position in context')\n",
    "    plt.ylabel(f'Embedding similarity')\n",
    "    plt.title(f'Relative to position {i}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Transformer blocks\n",
    "* **blocks** is a list (size 12) of dictionaries containing weights for each of the 12 transformer blocks in succession:\n",
    "  - **attn** is the attention layer parameters\n",
    "    - **c_attn** is the weights and biases that project the input embeddings to Q, K, V matrices (which are then used in the attention calculation)\n",
    "    - **c_proj** is the linear transformation applied to the output of the attention calculation\n",
    "  - **ln_1** and **ln_2** are the parameters of the two normalization layers in the block, similar to ln_f\n",
    "  - **mlp** is the conventional neural network layer at the end of the block\n",
    "    - **c_fc** is the weights and biases for the layer\n",
    "    - **c_proj** is the linear transformation applied to the output of the fully-connected layer   \n",
    "\n",
    "Run the code block below to view the structure of the parameters for the transformer blocks. You can also uncomment and modify the commented-out line to see the weights themselves; they are large arrays of floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4,depth=4)\n",
    "np.set_string_function(lambda x:f\"array with shape {x.shape}\") # override string representation of numpy array to just print shape\n",
    "pp.pprint(params['blocks'])\n",
    "np.set_string_function(None) # reset string representation back to default\n",
    "\n",
    "#print(\"\\n\")\n",
    "#print(params['blocks'][0]['attn']['c_attn']['w']) # Print the weight parameter of the attention layer in the first transformer block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Outputs\n",
    "The output of the model is a likelihood score for every possible token in the vocabulary. The picoGPT function used in part 1 above always chooses the token with the highest score, which is entirely repeatable/deterministic. For greater variety/'creativity' the scores can be converted into a probability distribution (e.g. using softmax) which is sampled randomly to choose the next token instead.\n",
    "\n",
    "Vary the prompt below to see how it changes the highest-scoring predicted tokens.\n",
    "\n",
    "As an extension exercise, you could try implementing a random sampler from the highest-scoring N tokens and seeing how its sentence completions compare to the deterministic approach in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from picoGPT.gpt2_pico import gpt2 as calculate_logits, softmax\n",
    "from collections import OrderedDict\n",
    "\n",
    "prompt = \"The cat sat on the\"\n",
    "\n",
    "tokens = tokenizer.encode(prompt)\n",
    "\n",
    "logits = calculate_logits(tokens, **params, n_head=hparams[\"n_head\"])\n",
    "next_token_scores = logits[-1]\n",
    "\n",
    "wordparts_and_scores = [(tokenizer.decode([i]),score) for (i,score) in enumerate(next_token_scores)]\n",
    "wordparts_and_scores = sorted(wordparts_and_scores,key=lambda x:x[1],reverse=True) # sort by score\n",
    "\n",
    "top5_wordparts_and_scores = wordparts_and_scores[:5]\n",
    "bottom5_wordparts_and_scores = reversed(wordparts_and_scores[-5:])\n",
    "\n",
    "print(f\"Top 5 predictions to complete '{tokenizer.decode(tokens)}':\")\n",
    "for (i,(word,score)) in enumerate(top5_wordparts_and_scores):\n",
    "    softmax_probability = np.exp(score)/np.sum(np.exp(next_token_scores))\n",
    "    print(f\"{i+1}. {word} (score = {score:.2f}, softmax probability = {softmax_probability:.2f})\")\n",
    "    \n",
    "print(f\"\\nBottom 5 predictions to complete '{tokenizer.decode(tokens)}':\")\n",
    "for (i,(word,score)) in enumerate(bottom5_wordparts_and_scores):\n",
    "    softmax_probability = np.exp(score)/np.sum(np.exp(next_token_scores))\n",
    "    print(f\"{i+1}. {word} (score = {score:.2f}, softmax probability = {softmax_probability:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 PicoGPT",
   "language": "python",
   "name": "python38-picogpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
